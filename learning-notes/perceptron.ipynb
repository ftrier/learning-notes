{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c3a91e",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477a584",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151218e",
   "metadata": {},
   "source": [
    "## Linear Classification Problem\n",
    "\n",
    "Let's start by generating a simple linear classification problem.  That is, a problem in which the optimal decision boundary is known to be linear.  We're also generating a problem in which the samples are perfectly linearly separable.  This is the exception rather than the rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d8365",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.random.random_sample((100,2))\n",
    "labels = (data[:,0]*0.7+data[:,1]*0.4>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e505a90",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d0 = data[labels==False]\n",
    "d1 = data[labels]\n",
    "plt.plot(d0[:,0],d0[:,1],\"bo\")\n",
    "plt.plot(d1[:,0],d1[:,1],\"ro\")\n",
    "print(len(d0),len(d1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b9217",
   "metadata": {},
   "source": [
    "We can visually read off the equation for the decision boundary.  It should run from (0.15,1.0) to about (0.7,0.0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fb50d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d0 = data[labels==False]\n",
    "d1 = data[labels]\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.plot(d0[:,0],d0[:,1],\"bo\")\n",
    "plt.plot(d1[:,0],d1[:,1],\"ro\")\n",
    "plt.plot([0.15,0.72],[1.0,0.0],\"g\") # guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2f116",
   "metadata": {},
   "source": [
    "Note that these decision boundaries are linear, by construction.  \n",
    "How do we find these decision boundaries automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b326f69",
   "metadata": {},
   "source": [
    "(augmented vectors)\n",
    "\n",
    "To simplify computations, we would like to use homogeneous coordinates.\n",
    "\n",
    "A linear decision boundary is given by a formula of the form:\n",
    "\n",
    "$$a_1 x_1 + ... + a_n x_n = d$$\n",
    "\n",
    "We can write this as\n",
    "\n",
    "$$a \\cdot x = d $$\n",
    "\n",
    "With this, we classify as class 0:\n",
    "\n",
    "$$a \\cdot x \\leq d$$\n",
    "\n",
    "and as class 1:\n",
    "\n",
    "$$a \\cdot x \\gt d$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2924c",
   "metadata": {},
   "source": [
    "(augmented vectors)\n",
    "\n",
    "However, it turns out to be inconvenient to have the $d$ in all our equations; we therefore turn this inhomogeneous problem into a homogeneous one by transforming the vectors.\n",
    "\n",
    "$$ x \\rightarrow (1,x) $$\n",
    "\n",
    "$$ a \\rightarrow (-d,x) $$\n",
    "\n",
    "Now we can write for our decision problem:\n",
    "\n",
    "$$ a \\cdot x \\leq 0$$\n",
    "\n",
    "and\n",
    "\n",
    "$$a \\cdot x \\gt 0$$\n",
    "\n",
    "Therefore, for any inhomogeneous linear decision problem, we can construct an equivalent homogeneous one by simply adding a column of 1's to the data vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276481c",
   "metadata": {},
   "source": [
    "## The Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d1d73",
   "metadata": {},
   "source": [
    "(sample correction)\n",
    "\n",
    "We want $a \\cdot x \\gt 1$ for all samples in class 1.  Now, assume that this isn't working for some sample $x$, $a \\cdot x \\leq 0$ even though it should be $\\gt 0$.  How do we fix that?\n",
    "\n",
    "Actuall, the solution is fairly simple:\n",
    "\n",
    "- When $a \\cdot x \\leq 0$ when it should be $\\gt 0$, we add \"a little bit of $x$\" to $a$.  \n",
    "- That is, we update $a \\rightarrow a + \\epsilon x$.  \n",
    "\n",
    "Then, next time, $(a + \\epsilon x) \\cdot x = a \\cdot x + \\epsilon ||x||^2 > a \\cdot x$.\n",
    "\n",
    "- For the other case, we subtract a little bit of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18dfdd",
   "metadata": {},
   "source": [
    "(flipping class 0 to class 1)\n",
    "\n",
    "Notice that the two cases are symmetrical.\n",
    "\n",
    "Instead of considering two classes, we can simplify the problem further by flipping the sign of $x$ for all the samples in class $0$.  \n",
    "\n",
    "Then, we require for all samples in the transformed problem that $a \\cdot x \\gt 0$.\n",
    "\n",
    "(Remember that we have also augmented the data vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523eeb62",
   "metadata": {},
   "source": [
    "(perceptron learning algorithm)\n",
    "\n",
    "So, the perceptron learning algorithm is:\n",
    "\n",
    "- given input samples $\\\\{x_1,...,x_N\\\\} \\subseteq R^n$ and corresponding classifications $\\\\{c_1,...,c_N\\\\} \\subseteq \\\\{0,1\\\\}$\n",
    "- replace all the $x_i$ with augmented vectors $x_i \\rightarrow (1,x_i)$\n",
    "- for every sample for which $c_i=0$, negate the corresponding $x_i$, that is $x_i \\rightarrow -x_i$\n",
    "- pick a random starting vector $a$\n",
    "- repeatedly iterate through the training samples $x_i$\n",
    "  - if $a \\cdot x_i > 0$ then continue\n",
    "  - otherwise update $a$ according to $a \\rightarrow a + \\epsilon x_i$\n",
    "  - stop if there are no more updates\n",
    "\n",
    "Actually, it turns out that we can just use $\\epsilon = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8e771",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmented = np.concatenate([np.ones((100,1)),data],axis=1)\n",
    "flipped = augmented.copy()\n",
    "flipped[labels==False] = -flipped[labels==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793206c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.random.random_sample((3))\n",
    "\n",
    "for epoch in range(100):\n",
    "    nchanged = 0\n",
    "    for i in range(len(flipped)):\n",
    "        if np.dot(a, flipped[i]) > 0:\n",
    "            continue\n",
    "        a += flipped[i]\n",
    "        nchanged += 1\n",
    "    if nchanged == 0:\n",
    "        break\n",
    "\n",
    "d,a0,a1 = a\n",
    "print(d, a0, a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353d2ee",
   "metadata": {},
   "source": [
    "The linear equation is now:\n",
    "\n",
    "$$a_0 x + a_1 y + d = 0$$\n",
    "\n",
    "For $x=0$:\n",
    "\n",
    "$$y = -\\frac{d}{a_1}$$\n",
    "\n",
    "For $y=0$:\n",
    "\n",
    "$$x = -\\frac{d}{a_0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afa4e7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d0 = data[labels==False]\n",
    "d1 = data[labels]\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.plot(d0[:,0],d0[:,1],\"bo\")\n",
    "plt.plot(d1[:,0],d1[:,1],\"ro\")\n",
    "plt.plot([0,-d/a0],[-d/a1,0],\"g\")\n",
    "print(len(d0),len(d1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ad622",
   "metadata": {},
   "source": [
    "That has worked rather well, but it leaves open a number of questions:\n",
    "\n",
    "- Is the algorithm guaranteed to stop?\n",
    "- How do we deal with more than two classes?\n",
    "- What is the error rate?\n",
    "- How does it relate to the c.c.d. $p(x|c)$?\n",
    "- What happens when the data isn't linearly separable?\n",
    "- It's fairly easy to show that the algorithm converges if the data is linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0318653",
   "metadata": {},
   "source": [
    "## The Perceptron Criterion Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16dd0a",
   "metadata": {},
   "source": [
    "(criterion functions)\n",
    "\n",
    "In the perceptron learning algorithm, we made a change to the weight vector every time a sample was misclassified.\n",
    "\n",
    "In essence, we have been trying to _minimize the number of misclassified samples_\n",
    "\n",
    "We call the number of misclassified samples $J(w) = \\hbox{\\# misclassified samples}$ a _criterion function_.\n",
    "\n",
    "Discriminative learning algorithms are frequently expressed in terms of optimization of criterion functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5165abc",
   "metadata": {},
   "source": [
    "(optimization algorithms)\n",
    "\n",
    "The number of misclassified samples is an inconvenient criterion function because it is piecewise constant (it is a collection of step functions), so its gradient is either zero or infinity.\n",
    "\n",
    "Most numerical optimization algorithms use gradients in some form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb025558",
   "metadata": {},
   "source": [
    "(perceptron criterion function)\n",
    "\n",
    "A better function is to sum up the total amount \n",
    "by which samples on the wrong side of the decision boundary are misclassified.\n",
    "\n",
    "This is zero when the data is separated, but positive if any samples are misclassified.\n",
    "\n",
    "In formulas:\n",
    "\n",
    "$$J_p(w) = \\sum_{i\\in{\\cal E}_w} -w\\cdot x_i$$\n",
    "\n",
    "where\n",
    "\n",
    "$${\\cal E}_w = { i | w \\cdot x < 0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7f7a9",
   "metadata": {},
   "source": [
    "(gradient descent optimization)\n",
    "\n",
    "We can optimize this function by _gradient descent_.\n",
    "That is, we iteratively update the weight vector $w$ by \n",
    "adding a small multiple of the negative of the\n",
    "gradient $\\nabla J_p(w)$ to it.  What is the gradient?\n",
    "\n",
    "$$\\nabla J_p(w) = \\nabla \\sum_{i\\in{\\cal E}_w} -w\\cdot x_i = \\sum_{i\\in{\\cal E}_w} -x_i$$\n",
    "\n",
    "Therefore, our update rule becomes:\n",
    "\n",
    "$$ w \\leftarrow w + \\eta \\sum_{i\\in{\\cal E}_w} x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4c959",
   "metadata": {},
   "source": [
    "(stochastic vs batch gradient descent)\n",
    "\n",
    "This is a _batch update_ rule; that is, we add up all the gradients\n",
    "for each misclassified samples, and only then update the total weight vector.\n",
    "That is inefficient and turns out to be unnecessary in this case anyway.\n",
    "\n",
    "A _stochastic gradient descent_ rule updates after every misclassified sample via\n",
    "\n",
    "$$ w \\leftarrow w + \\eta x_i $$\n",
    "\n",
    "In the case of perceptron learning,\n",
    "this is also called the single sample correction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a5c25",
   "metadata": {},
   "source": [
    "## Proof of Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf12cd8",
   "metadata": {},
   "source": [
    "Let's consider a particularly simple case for the single sample\n",
    "correction perceptron learning algorithm: the case where $\\eta = 1$.  Proving convergence for that case shows that for any separable learning problem, there exists a known sequence of $\\eta$ (namely all 1s) that make perceptron learning converge. \n",
    "\n",
    "We also assume that the training vectors are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724f583",
   "metadata": {},
   "source": [
    "Assume that we're updating the weight vector in a sequence of updates; \n",
    "we number the updates as $\\tau=1,2,...$ (we don't need to worry about \n",
    "the vectors that are classified correctly).\n",
    "Let $\\hat{w}$ be a solution vector.\n",
    "Consider now the distance of $w(\\tau+1)$ from some multiple of the solution vector:\n",
    "\n",
    "$$w(\\tau+1) - \\alpha \\hat{w} = (w(\\tau)-\\alpha \\hat{w}) + x_\\tau$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0047f",
   "metadata": {},
   "source": [
    "Now square both sides:\n",
    "\n",
    "$$||w(\\tau+1) - \\alpha \\hat{w}||^2 = ||(w(\\tau)-\\alpha \\hat{w}) + x_\\tau||^2 = ||w(\\tau)-\\alpha \\hat{w}||^2 - 2(w(\\tau)-\\alpha\\hat{w})\\cdot x_\\tau+||x_\\tau^2||$$\n",
    "\n",
    "$$ = ||w(\\tau)-\\alpha \\hat{w}||^2 - 2 w(\\tau)\\cdot x_\\tau - 2 \\alpha\\hat{w} \\cdot x_\\tau + ||x_\\tau^2||$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d6158",
   "metadata": {},
   "source": [
    "We know that $w(\\tau) \\cdot x_\\tau < 0$ because it was misclassified, so\n",
    "\n",
    "$$||w(\\tau+1) - \\alpha \\hat{w}||^2 \\leq ||w(\\tau)-\\alpha \\hat{w}||^2 - 2 \\alpha\\hat{w} \\cdot x_\\tau + ||x_\\tau^2||$$\n",
    "\n",
    "We also know that $\\hat{w}\\cdot x_\\tau>0$ (because $\\hat{w}$) is a solution.\n",
    "So, we can choose some $\\alpha$ such that we get a provable reduction\n",
    "in the distance of the weight vector from the true vector.  To choose that, let...\n",
    "\n",
    "$$ \\beta^2 = \\max_i ||x_i||^2$$\n",
    "\n",
    "$$ \\gamma = \\min \\hat{w} x_i $$\n",
    "\n",
    "(This is greater than zero for all vectors.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878155ea",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "$$||w(\\tau+1f) - \\alpha \\hat{w}||^2 \\leq ||w(\\tau)-\\alpha \\hat{w}||^2 - 2 \\alpha\\gamma + \\beta^2$$\n",
    "\n",
    "If we choose $\\alpha = \\frac{\\beta^2}{\\gamma}$, then\n",
    "\n",
    "$$||w(\\tau+1) - \\alpha \\hat{w}||^2 \\leq ||w(\\tau)-\\alpha \\hat{w}||^2 - \\beta^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b93a2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This means that after $k$ steps, we have a reduction by $k\\beta^2$ in the error.  Since the distance can't become negative, we have a bound on the number of corrections of:\n",
    "\n",
    "$$\\hat{k} = \\frac{||w(1) - \\alpha \\hat{w}||^2}{\\beta^2}$$\n",
    "\n",
    "Let's pick $w(1)=0$, then we get:\n",
    "\n",
    "$$\\hat{k} = \\frac{\\alpha^2 ||\\hat{w}||^2}{\\beta^2} = \\frac{\\beta^2 ||\\hat{w}||}{\\gamma^2} = \\frac{\\max ||x_i||^2 ||\\hat{w}||^2}{\\min (x_i\\cdot \\hat{w})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed043a",
   "metadata": {},
   "source": [
    "## Non-Separable Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065b27b",
   "metadata": {},
   "source": [
    "If classes are not separable, things get more difficult.  In fact, it's not even clear what kind of solution we are looking for in that case.\n",
    "\n",
    "The most obvious solution we might want, the solution that has the minimum number of samples \"on the wrong side\" of the decision boundary is computationally hard to find.\n",
    "\n",
    "There are, however, other kinds of criteria we might apply:\n",
    "\n",
    "- Minimize the perceptron criterion function.\n",
    "- Find a good \"least square approximation\", that is, try to minimize $\\sum_i |a\\cdot x_i - c_i|^2$ (note that we cannot use the sign flip in this case).\n",
    "- Find a good \"least square approximation\", for some other objective function, as in $\\sum_i f(a \\cdot x_i - c_i)$ (note that we cannot use the sign flip in this case).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
