{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Clustering is a fundamental technique in unsupervised machine learning that aims to group similar data points together based on their characteristics. One popular clustering algorithm is k-means, which partitions data into k distinct clusters.\n",
    "\n",
    "The k-means algorithm starts by randomly initializing k cluster centroids. It then iteratively assigns each data point to the nearest centroid and updates the centroids based on the mean of the assigned points. This process continues until convergence, where the centroids no longer change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "K-means clustering is widely used in various domains, such as customer segmentation, image compression, anomaly detection, and recommendation systems. It provides a simple and efficient way to discover patterns and structure within datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step, as always, is to get us some data. We are going to use the scikit library to generate us blobs of data. The red dots depict the cluster centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "X, y, centers = make_blobs(n_samples=500, centers=k, return_centers=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1])\n",
    "plt.plot(centers[:, 0], centers[:, 1], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all partitions $ C_0 \\cup C_1 ... \\cup C_k $, we find the one that minimizes:\n",
    "\n",
    "$$ \\min*{C_0 \\cup C_1 ... \\cup C_k} \\sum*{i=1}^{k} \\sum*{x \\in C_i} ||x_i - \\frac{1}{|C_i|} \\sum*{x_j \\in C_i} x_j ||^2 $$\n",
    "\n",
    "- $ k $ are the number of clusters\n",
    "- $ C_i $ is the i-th partition\n",
    "- $ | C_i | $ is the number of samples belonging to $ C_i $\n",
    "- $ C_0 \\cup C_1 ... \\cup C_k $ is the union of all partitions\n",
    "- $ x \\in C_i $ means x in partition $ C_i $\n",
    "\n",
    "It calculates the sum of the squared distances between each data point and the mean of its assigned cluster. The goal is to minimize this sum by finding the optimal partitioning of the data into clusters.\n",
    "For the algorithm below, you can safely ignore this equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, max_iter=100):\n",
    "    # Choose k random samples as center of clusters\n",
    "    centroids = X[np.random.choice(len(X), k, replace=False)]\n",
    "    for i in range(max_iter):\n",
    "        # Compute distance of each sample to each centroid\n",
    "        dist = np.linalg.norm(X[:, None] - centroids, axis=2)\n",
    "        # Assign each sample to the closest centroid\n",
    "        labels = np.argmin(dist, axis=1)\n",
    "        # Compute the new centroids by taking the mean of all samples assigned to each centroid\n",
    "        new_centroids = np.array(\n",
    "            [X[labels == i].mean(axis=0) for i in range(k)])\n",
    "        if np.all(centroids == new_centroids):\n",
    "            print(f'Converged after {i} iterations')\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, centroids = kmeans(X_train, k)\n",
    "centroids = centroids[np.argsort(centroids[:, 0])]\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=labels)\n",
    "plt.plot(centers[:, 0], centers[:, 1], 'ro')\n",
    "plt.plot(centroids[:, 0], centroids[:, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our clusters (green dots), we can use them for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(X, centroids):\n",
    "    dist = np.linalg.norm(X[:, None] - centroids, axis=2)\n",
    "    return np.argmin(dist, axis=1)\n",
    "\n",
    "\n",
    "labels = classify(X_test, centroids)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "\n",
    "1. You have to figure k out by yourself, although there are methods like the elbow method to help you with that.\n",
    "1. We generated a blob dataset, because the shape of the data matters for k-means. If the data were not spherical, k-means would be a poor choice.\n",
    "1. k-means cannot handle skewed data very well. By its nature, the centroids will always prefer bigger clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method\n",
    "\n",
    "We iterate over multiple different values of $k$ and look for the sweet spot, where the sum of distances between the samples and their cluster's centroid starts to become low. The name becomes obvious if we look at the shape of the resulting graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for k in range(1, 10):\n",
    "    labels, centroids = kmeans(X_train, k)\n",
    "    inertia.append(np.linalg.norm(X_train - centroids[labels]))\n",
    "\n",
    "plt.plot(range(1, 10), inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-spherical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "X, y = make_moons(n_samples=50)\n",
    "labels, centroids = kmeans(X, k)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.plot(centroids[:, 0], centroids[:, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, centers = make_blobs(n_samples=[25, 50, 200], cluster_std=[\n",
    "                           0.1, 0.1, 0.2], centers=[(0.1, 0.5), (0.4, 0), (0.6, 0.6)], return_centers=True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.plot(centers[:, 0], centers[:, 1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, centroids = kmeans(X, 3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.plot(centroids[:, 0], centroids[:, 1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, k-means does not handle non-spherical data well and skewed data might also create a problem.\n",
    "\n",
    "Nevertheless its simplicity and ease of use is often one of the first choices to cluster data. There are excellent [libraries](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for this available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
