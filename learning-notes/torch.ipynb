{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "While it's important to be familiar with specific frameworks, I believe it's not beneficial to focus too much on any one in particular, as they often evolve or get replaced over time. We've seen this with Caffe, Theano, Tensorflow, Keras, and others. They all still exist, but currently, PyTorch is the most popular. If you're interested in learning PyTorch, there are many comprehensive tutorials and guides available that are more detailed than this one.\n",
    "\n",
    "In this guide, I aim to highlight the parallels between our custom-built MLP and an MLP implemented using PyTorch. We are going to inspect the same network architectures on the same data, that is why:\n",
    "\n",
    "**I strongly suggest going through the MLP notebook before proceeding with this one.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this repo, we only want the CPU version of PyTorch to keep the image size down\n",
    "# Unfortunately, the default PyTorch package includes the CUDA version, so we need to install it from a different source\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data():\n",
    "    data = np.random.random_sample((100, 2))\n",
    "    labels = (data[:, 0]-data[:, 1] < 0.3)\n",
    "\n",
    "    d0 = data[labels == False]\n",
    "    d1 = data[labels]\n",
    "\n",
    "    targets = np.array([labels, 1-labels]).T\n",
    "    return data, targets, labels, d0, d1\n",
    "\n",
    "\n",
    "data, targets, labels, d0, d1 = random_data()\n",
    "plt.plot(d0[:, 0], d0[:, 1], \"bo\")\n",
    "plt.plot(d1[:, 0], d1[:, 1], \"ro\")\n",
    "\n",
    "# Plot the decision boundary of a classifier: y = x - 0.3\n",
    "x = np.linspace(0.3, 1, 2)\n",
    "y = x - 0.3\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.relu(self.fc1(x))\n",
    "        out = nn.functional.sigmoid(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "network = MLP(input_size=2, hidden_size=5, num_classes=2)\n",
    "print(\"Network Structure:\", network)\n",
    "\n",
    "random_prediction = network(torch.tensor(data, dtype=torch.float32))\n",
    "print(\"Random Prediction:\", random_prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully created a Multilayer Perceptron (MLP) using PyTorch, and it's structured similarly to the MLP we built from scratch. Let's dissect what we've done:\n",
    "\n",
    "1. We've created two fully connected layer objects. These are objects because they need to store certain information, such as gradients, which are essential for the learning process. This is similar to what we did in our custom-built MLP.\n",
    "2. The activation functions ReLU and Sigmoid are used directly as function calls. They don't need to store any state information, so they don't need to be objects. They simply take the output of one layer and transform it to be the input for the next layer.\n",
    "3. To get the output of our network for a given input, we simply call `network(...)` with the input. This runs the input through all the layers and activation functions of the network.\n",
    "4. One of the advantages of using PyTorch is that we can feed all our data into the network at once. PyTorch, like most other deep learning frameworks, is designed to work with batches of data, allowing us to process multiple data points simultaneously. This can lead to significant speedups in training.\n",
    "\n",
    "PyTorch provides a lot of functionality out of the box, like automatic differentiation and batch processing, which makes it easier and more efficient to work with, as you will see later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(f, d0, d1, diag=None):\n",
    "    diag = diag or plt\n",
    "    xx, yy = np.meshgrid(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01))\n",
    "    grid_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        zz = np.array([f(torch.tensor(p, dtype=torch.float32))\n",
    "                      for p in grid_data]).reshape(xx.shape)\n",
    "        diag.contourf(xx, yy, zz, alpha=0.3)\n",
    "        diag.plot(d0[:, 0], d0[:, 1], \"bo\")\n",
    "        diag.plot(d1[:, 0], d1[:, 1], \"ro\")\n",
    "\n",
    "\n",
    "decision_boundary(lambda p: network(p)[0], d0, d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we do the same as in the MLP notebook, so we plan to create a classifier, which can separate two classes. We have used a simple gradient descent method, where we adapted our parameters using a learning rate. As Loss function we had used, the Mean Squared Error loss.\n",
    "\n",
    "$$ MSE = \\frac{1}{2}(y - y_t)^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(network.parameters(), lr=1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# We convert the data to PyTorch tensors\n",
    "input_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(500):  # Epoch is the number of times we go through the entire dataset\n",
    "    optimizer.zero_grad()  # weight_update = 0, bias_update = 0\n",
    "    outputs = network(input_tensor)\n",
    "    loss = criterion(outputs, target_tensor)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()  # Compute the gradients\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "with torch.no_grad():\n",
    "    correct = (network(input_tensor).argmax(dim=1) ==\n",
    "               target_tensor.argmax(dim=1)).sum().item()\n",
    "    print(\"Correct predicted: \", 1.0 * correct / len(data))\n",
    "    decision_boundary(lambda p: network(p)[0], d0, d1, ax[0])\n",
    "\n",
    "ax[1].set_title(\"Loss\")\n",
    "ax[1].plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the effectiveness of our PyTorch implementation, it's beneficial to compare these graphs with those from the original notebook where we implemented the MLP from scratch.\n",
    "\n",
    "The graphs should appear quite similar because the parameters we used for training the model in both cases are mostly identical. Specifically, we trained our model for 500 epochs in both notebooks using the same gradient descent approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_random_data(radius=0.40):\n",
    "    data = np.random.random_sample((1000, 2))\n",
    "    labels = (np.linalg.norm(data - 0.5, axis=1) < radius)\n",
    "\n",
    "    d0 = data[labels == False]\n",
    "    d1 = data[labels]\n",
    "\n",
    "    # The value range of tanh is -1 to 1\n",
    "    # -1 for negative class, 1 for positive class\n",
    "    targets = 2*labels - 1\n",
    "    return data, targets, labels, d0, d1\n",
    "\n",
    "\n",
    "data, targets, labels, d0, d1 = circular_random_data()\n",
    "plt.plot(d0[:, 0], d0[:, 1], \"bo\")\n",
    "plt.plot(d1[:, 0], d1[:, 1], \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, ):\n",
    "        super(CircularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.tanh(self.fc1(x))\n",
    "        out = nn.functional.tanh(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "network = CircularMLP(input_size=2, hidden_size=4, num_classes=1)\n",
    "print(\"Network Structure:\", network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.05)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(targets, dtype=torch.float32).view(-1, 1)\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Target Tensor Shape:\", target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "batch_size = 10\n",
    "for epoch in range(500):\n",
    "    loss_value = 0\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(input_tensor[i:i+batch_size])\n",
    "        loss = criterion(outputs, target_tensor[i:i+batch_size])\n",
    "        loss_value += loss.item() / len(data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss_value)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "with torch.no_grad():\n",
    "    correct = np.sum([x == y for x, y in zip(np.sign(network(input_tensor)[:, 0]), target_tensor[:, 0])])\n",
    "    print(\"Correct predicted: \", 1.0 * correct / len(data))\n",
    "    decision_boundary(lambda p: network(p), d0, d1, ax[0])\n",
    "\n",
    "ax[1].set_title(\"Loss\")\n",
    "ax[1].plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your everyday tasks, it's rare that you'll need to build a neural network from the ground up. There's usually no necessity for this because there are many well-established libraries. These tools are designed to handle the complex mathematics and optimization processes involved in neural networks, which makes them less prone to errors compared to a custom-built solution.\n",
    "\n",
    "However, having a fundamental understanding of the underlying concepts of neural networks is incredibly beneficial. This knowledge can be particularly useful when you're debugging code. If something goes wrong while training a model or making predictions, understanding how the network operates can help you identify where the issue lies. For example, you might be able to recognize if the problem is due to incorrect data preprocessing, a poorly chosen activation function, or an issue with the way the network's layers are connected.\n",
    "\n",
    "In summary, while it's not common to implement a neural network from scratch in your daily work, having a solid grasp of the principles behind neural networks can greatly assist in debugging and optimizing your models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
