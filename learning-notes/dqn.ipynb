{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "This notebook was kindly provided by [Nilau1998](https://github.com/Nilau1998).\n",
    "\n",
    "The following Deep Q-Learning algorithm for the Cart Pole problem provided by OpenAIs Gymnasium was implemented using the PyTorch library. The main components of the implementation are:\n",
    "\n",
    "\n",
    "1. **Experience Replay:**\n",
    "   - A replay buffer was implemented to store the agent's experiences, consisting of the state, action, reward, next state, and done flag.\n",
    "   - Mini-batches were sampled from the replay buffer during training to update the neural network, breaking the correlation between consecutive experiences and improving the stability of the learning process.\n",
    "\n",
    "2. **Epsilon-Greedy Exploration:**\n",
    "   - An epsilon-greedy policy was used to balance exploration and exploitation. Initially, the exploration rate (epsilon) was set high to encourage exploration.\n",
    "   - Epsilon was gradually reduced over time, allowing the agent to exploit its learned policy more frequently. This annealing schedule helps transition from exploration to exploitation as training progresses.\n",
    "\n",
    "3. **Neural Network:**\n",
    "   - The neural network approximates the Q-values for different state-action pairs. It consists of an input layer for the state representation, hidden layers with ReLU activations, and an output layer predicting the Q-values for each possible action.\n",
    "   - The architecture and size of the network were chosen to balance computational efficiency and learning capability for the Cart Pole problem.\n",
    "\n",
    "4. **Target Network:**\n",
    "   - To stabilize the training process, a target network was used. This separate neural network has the same architecture as the main network.\n",
    "   - The weights of the target network are periodically updated with the weights of the main network, reducing oscillations and divergence in the learning process by providing a more stable target for the Q-value updates.\n",
    "\n",
    "5. **Loss Function and Optimization:**\n",
    "   - The loss function used is the mean squared error (MSE) between the predicted Q-values and the target Q-values.\n",
    "   - The target Q-values are computed using the reward and the discounted maximum Q-value of the next state, as predicted by the target network.\n",
    "   - The Adam optimizer was used to update the neural network parameters based on the computed loss, chosen for its adaptive learning rate properties that efficiently handle the gradients during training.\n",
    "\n",
    "This implementation ensures that the agent learns to balance the pole on the cart by optimizing the expected cumulative reward through interaction with the environment, guided by the Q-learning algorithm and neural network function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike \\textbf{policy gradient} methods, which attempt to learn functions which directly map an observation to an action \\textbf{Q-Learning} attempts to learn the value of being in a given state, and taking a specific action there. In it's simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We make updates to our Q-table using something called the \\textbf{Bellman equation}, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state, e.g. \\(Q(s,a) = r + y(max(Q(s',a'))\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer\n",
    "\n",
    "The `ReplayBuffer` is designed to store and manage experiences for training a reinforcement learning agent. It helps in stabilizing and improving the learning process. By randomly selecting a batch of experiences, it helps to break correlations between consecutive experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.memory = collections.deque([], maxlen=size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return [torch.tensor(t).to(device) for t in list(zip(*batch))]\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy-Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epsilon_start, epsilon_end, epsilon_decay=1000):\n",
    "    return lambda episode: epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * episode / epsilon_decay)\n",
    "eps_decay = decay(0.9, 0.1, 1000)\n",
    "\n",
    "plt.plot([eps_decay(i) for i in range(5000)])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network\n",
    "\n",
    "- **Architecture**: The neural network consists of three fully connected layers. The first two layers use ReLU activation functions.\n",
    "  - `fc1`: Input layer to the first hidden layer with 128 units.\n",
    "  - `fc2`: First hidden layer to the second hidden layer with 64 units.\n",
    "  - `fc3`: Second hidden layer to the output layer, with a number of units equal to the number of actions.\n",
    "- **Optimizer**: The Adam optimizer is used with a learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, lr):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(*input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = nn.functional.relu(self.fc1(state))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        return self.forward(state).argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Agent Class**\n",
    "\n",
    "- **Initialization**: Initializes the hyperparameters and components such as epsilon for exploration, gamma for discount factor, and two networks (the main Q-network and the target network). Also initializes the replay buffer.\n",
    "- **Action Selection (choose_action)**: Uses an epsilon-greedy policy to select actions.\n",
    "  - Epsilon is decayed over time to shift from exploration to exploitation.\n",
    "  - With probability `epsilon_threshold`, selects a random action; otherwise, selects the action with the highest predicted Q-value.\n",
    "- **Learning (learn)**: Updates the Q-network using experiences sampled from the replay buffer.\n",
    "  - If there are not enough experiences, the learning process is skipped.\n",
    "  - Calculates predicted Q-values for the current state-action pairs.\n",
    "  - Calculates target Q-values using rewards and the maximum Q-value for the next state, as predicted by the target network.\n",
    "  - Computes the loss using mean squared error between predicted and target Q-values, and performs a backward pass to update the network weights.\n",
    "  - Periodically updates the target network parameters to stabilize training.\n",
    "- **Network Update (update_network_parameters)**: Softly updates the target network parameters using a factor `tau` to blend the weights of the main Q-network and the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, epsilon, gamma, input_shape, n_actions, tau=0.05):\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.dqn = DeepQNetwork(input_shape, n_actions, 0.001)\n",
    "        self.target = DeepQNetwork(input_shape, n_actions, 0.001)\n",
    "        self.tau = tau\n",
    "\n",
    "    def choose_action(self, observation, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return self.dqn.predict(observation)\n",
    "        return np.random.choice(self.n_actions)\n",
    "\n",
    "    def learn(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        q_pred = self.dqn.forward(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        q_next = self.target.forward(next_states).detach().max(dim=1).values\n",
    "        q_next[dones] = 0.0\n",
    "        q_target = rewards + (self.gamma * q_next)\n",
    "\n",
    "        loss = nn.functional.mse_loss(q_target, q_pred).to(device)\n",
    "        self.dqn.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.dqn.optimizer.step()\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        target_value_state_dict = self.target.state_dict()\n",
    "        value_state_dict = self.dqn.state_dict()\n",
    "        for name in value_state_dict:\n",
    "            target_value_state_dict[name] = value_state_dict[name] * \\\n",
    "                self.tau + target_value_state_dict[name]*(1-self.tau)\n",
    "        self.target.load_state_dict(target_value_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the Training Loop and Performance Visualization\n",
    "\n",
    "The provided script trains a Deep Q-Learning agent on the CartPole-v1 environment using the `Agent` and `ReplayBuffer` classes. The key steps in the training loop and performance visualization are:\n",
    "\n",
    "#### **Environment Setup and Agent Initialization**\n",
    "\n",
    "- The CartPole-v1 environment is created using the `gymnasium` library.\n",
    "- An `Agent` instance is initialized with specified hyperparameters:\n",
    "  - `epsilon=0.9` for the initial exploration rate.\n",
    "  - `gamma=0.99` for the discount factor.\n",
    "  - `input_shape` as the shape of the environment's observation space.\n",
    "  - `n_actions` as the number of possible actions in the environment.\n",
    "\n",
    "#### **Training Loop**\n",
    "\n",
    "- The agent is trained over `n_games = 200` episodes.\n",
    "- For each episode:\n",
    "  - The environment is reset, and the initial observation is obtained.\n",
    "  - The `done` and `truncated` flags are set to `False`.\n",
    "  - A while loop runs until the episode is completed (`done` or `truncated` is `True`).\n",
    "  - Within the loop:\n",
    "    - The agent selects an action using the `choose_action` method.\n",
    "    - The environment is stepped with the chosen action to get the next state, reward, and completion flags.\n",
    "    - The transition (current state, action, reward, next state, done) is stored in the agent's replay buffer.\n",
    "    - The agent performs a learning step using the `learn` method.\n",
    "    - The observation is updated to the new state.\n",
    "  - The episode's score (total reward) is recorded.\n",
    "  - The current epsilon value is recorded.\n",
    "  - The average score over the last 100 episodes is calculated and recorded.\n",
    "  - Optional: The progress of the training can be printed every few episodes.\n",
    "\n",
    "#### **Performance Visualization**\n",
    "\n",
    "- **Scores Plot**: Shows the scores and average scores over the training episodes.\n",
    "  - The scores of each episode are plotted.\n",
    "  - The average scores over the last 100 episodes are plotted to show the trend.\n",
    "- **Epsilons Plot**: Shows the decay of the epsilon value over the training episodes.\n",
    "  - Epsilon values are scaled by 100 for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(epsilon=0.9, gamma=0.99, input_shape=env.observation_space.shape, n_actions=env.action_space.n)\n",
    "scores, epsilons, avg_scores = [], [], []\n",
    "for i in range(300):\n",
    "    score = 0\n",
    "    done, truncated = False, False\n",
    "    observation = env.reset()[0]\n",
    "    step = 0\n",
    "    while not done and not truncated:\n",
    "        action = agent.choose_action(observation, eps_decay(step))\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.memory.store_transition([observation, action, reward, observation_, done])\n",
    "        agent.learn(batch_size=64)\n",
    "        observation = observation_\n",
    "        step += 1\n",
    "\n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    avg_scores.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores, label='Scores')\n",
    "plt.plot(avg_scores, label='Average Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The model training stops when it consistently achieves the maximum score and the environment truncates. As expected, the average score continuously increases during training, indicating that the agent is effectively learning to control the CartPole. The epsilon value gradually decreases, meaning the agent relies more on its learned knowledge as training progresses.\n",
    "\n",
    "During training, it was observed that the number of episodes significantly impacts performance and required fine-tuning. The model presented here needed 200 episodes, but using more episodes sometimes led to catastrophic forgetting, where the model's performance deteriorated in later episodes. Automating this tuning process is challenging. One idea was to track the average score and stop training if it decreases, but this could prematurely halt training if the model is temporarily stuck in a local minimum and might improve in subsequent episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Script\n",
    "\n",
    "This script evaluates the trained agent on 100 episodes of the CartPole-v1 environment to assess its performance. The key steps are:\n",
    "\n",
    "- Resetting the environment at the start of each episode.\n",
    "- Using the trained agent to select actions based on observations.\n",
    "- Accumulating the score for each episode.\n",
    "- Recording and printing the score for each evaluation episode.\n",
    "- Calculating and printing the average score over all evaluation episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_games = 100\n",
    "\n",
    "eval_scores = []\n",
    "for i in range(n_eval_games):\n",
    "    score = 0\n",
    "    done, truncated = False, False\n",
    "    observation = env.reset()[0]\n",
    "    while not done and not truncated:\n",
    "        action = agent.choose_action(observation, 0)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        observation = observation_\n",
    "    eval_scores.append(score)\n",
    "\n",
    "plt.plot(eval_scores)\n",
    "plt.plot([np.mean(eval_scores)] * n_eval_games, linestyle='--')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Scores')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
