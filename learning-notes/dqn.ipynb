{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "This notebook was kindly provided by [Nilau1998](https://github.com/Nilau1998)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# To keep the image size small, we only want the CPU version of PyTorch.\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "\n",
    "Let us recap policy learning first. Policy learning attempts to learn functions which directly map an observation to an action:\n",
    "Our agent had three methods, `act`, `discount_rewards` and `train`.\n",
    "The agent runs a complete episode and stores all the rewards. After the episode is done, the rewards are weighted. Later rewards are weighted bigger than earlier rewards. The agent uses these rewards to calculate the gradient and optimize its network.\n",
    "\n",
    "Unlike policy learning, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. In it's simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. We make updates to our Q-table using something called Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.\n",
    "\n",
    "$$Q(s,a) = r + y \\max Q(s',a')$$\n",
    "\n",
    "* $Q(s, a)$ This represents the Q-value, which is the expected cumulative reward for taking action 'a' in state 's'. It is a function that maps a state-action pair to a value.\n",
    "* $r$ This represents the immediate reward obtained when taking action 'a' in state 's'.\n",
    "* $y$ This is the discount factor, which determines the importance of future rewards compared to immediate rewards. It is a value between 0 and 1.\n",
    "* $\\max Q(s', a')$: This represents the maximum Q-value for the next state 's' and all possible actions 'a'. It represents the estimated maximum future reward that can be obtained from the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer\n",
    "\n",
    "The `ReplayBuffer` is designed to store and manage experiences for training a reinforcement learning agent. It helps in stabilizing and improving the learning process. By randomly selecting a batch of experiences, it helps to break correlations between consecutive experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.memory = collections.deque([], maxlen=size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy-Policy\n",
    "\n",
    "An epsilon-greedy policy is used to balance exploration and exploitation. Initially, the exploration rate (epsilon) is high to encourage exploration.\n",
    "It is gradually reduced over time, allowing the agent to exploit its learned policy more frequently. This annealing schedule helps transition from exploration to exploitation as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epsilon_start, epsilon_end, epsilon_decay=1000):\n",
    "    return lambda episode: epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * episode / epsilon_decay)\n",
    "eps_decay = decay(0.9, 0.1, 1000)\n",
    "\n",
    "plt.plot([eps_decay(i) for i in range(5000)])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network\n",
    "\n",
    "- **Architecture**: The neural network consists of three fully connected layers. The first two layers use ReLU activation functions.\n",
    "  - `fc1`: Input layer to the first hidden layer with 128 units.\n",
    "  - `fc2`: First hidden layer to the second hidden layer with 64 units.\n",
    "  - `fc3`: Second hidden layer to the output layer, with a number of units equal to the number of actions.\n",
    "- **Optimizer**: The Adam optimizer is used with a learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, lr):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(*input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = nn.functional.relu(self.fc1(state))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        return self.forward(state).argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agent\n",
    "\n",
    "The agent has two networks: `dqn` and `target`. The networks have the same architecture and and calculate the the $Q(s,a)$. Basically, How good is action $a$ in state $s$. But they are used slightly different. `dqn` calculates the immediate Q-value while the `target` network is used to calculate $\\max Q(s',a')$.\n",
    "\n",
    "`act`: Uses an epsilon-greedy policy to select actions. Epsilon is decayed over time to shift from exploration to exploitation. Initially it is more likely to choose a random action.\n",
    "\n",
    "`train` : Updates the Q-network `dqn` using experiences sampled from the replay buffer. As loss function, we are using mean squared error between predicted and target Q-values. Periodically we soft update the `target` network parameters to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, input_shape, n_actions, tau=0.05):\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.dqn = DeepQNetwork(input_shape, n_actions, 0.001)\n",
    "        self.target = DeepQNetwork(input_shape, n_actions, 0.001)\n",
    "        self.tau = tau\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.random() > epsilon:\n",
    "            return self.dqn.predict(state)\n",
    "        return np.random.choice(self.n_actions)\n",
    "\n",
    "    def train(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        # sample random batch from memory\n",
    "        batch = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "\n",
    "        # calculate the q values for the current state and the next state\n",
    "        q_pred = self.dqn.forward(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        q_next = self.target.forward(next_states).detach().max(dim=1).values\n",
    "        # if the next state is terminal, the q value for the next state is 0\n",
    "        q_next[dones] = 0.0\n",
    "        q_target = rewards + (self.gamma * q_next)\n",
    "\n",
    "        loss = nn.functional.mse_loss(q_target, q_pred).to(device)\n",
    "        self.dqn.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.dqn.optimizer.step()\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        # update the target network using the soft update\n",
    "        target_value_state_dict = self.target.state_dict()\n",
    "        value_state_dict = self.dqn.state_dict()\n",
    "        for name in value_state_dict:\n",
    "            target_value_state_dict[name] = value_state_dict[name] * \\\n",
    "                self.tau + target_value_state_dict[name]*(1-self.tau)\n",
    "        self.target.load_state_dict(target_value_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(gamma=0.99, input_shape=env.observation_space.shape, n_actions=env.action_space.n)\n",
    "scores, epsilons, avg_scores = [], [], []\n",
    "step = 0\n",
    "for i in range(200):\n",
    "    score = 0\n",
    "    done, truncated = False, False\n",
    "    state = env.reset()[0]\n",
    "    while not done and not truncated:\n",
    "        action = agent.act(state, eps_decay(step))\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.memory.store_transition([state, action, reward, next_state, done])\n",
    "        agent.train(batch_size=64)\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    avg_scores.append(avg_score)\n",
    "\n",
    "plt.plot(scores, label='Scores')\n",
    "plt.plot(avg_scores, label='Average Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The model training stops when it consistently achieves the maximum score and the environment truncates. As expected, the average score continuously increases during training, indicating that the agent is effectively learning to control the CartPole. The epsilon value gradually decreases, meaning the agent relies more on its learned knowledge as training progresses.\n",
    "\n",
    "During training, it was observed that the number of episodes significantly impacts performance and required fine-tuning. The model presented here needed 200 episodes, but using more episodes sometimes led to catastrophic forgetting, where the model's performance deteriorated in later episodes. Automating this tuning process is challenging. One idea was to track the average score and stop training if it decreases, but this could prematurely halt training if the model is temporarily stuck in a local minimum and might improve in subsequent episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_games = 100\n",
    "eval_scores = []\n",
    "for i in range(n_eval_games):\n",
    "    score = 0\n",
    "    done, truncated = False, False\n",
    "    observation = env.reset()[0]\n",
    "    while not done and not truncated:\n",
    "        action = agent.act(observation, 0)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        observation = observation_\n",
    "    eval_scores.append(score)\n",
    "\n",
    "plt.plot(eval_scores)\n",
    "plt.plot([np.mean(eval_scores)] * n_eval_games, linestyle='--')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Scores')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
