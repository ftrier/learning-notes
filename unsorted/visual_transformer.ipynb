{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Transformer\n",
    "\n",
    "![Structure](images/visual_transformer/visual_transformer.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1224e1f50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "    assert h == w\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i+1) * patch_size,\n",
    "                              j * patch_size: (j+1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Patch](images/visual_transformer/visual_transformer_patch.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            if j % 2 == 0:\n",
    "                result[i][j] = np.cos(i / (10000 ** ((j-1)/d)))\n",
    "            else:\n",
    "                result[i][j] = np.sin(i / (10000 ** (j/d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "* x = token_dim\n",
    "* y = patches\n",
    "* differs in y direction\n",
    "\n",
    "![Positional Encoding](images/visual_transformer/positional_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put: we want, for a single image, each patch to get updated based on some similarity measure with the other patches. We do so by linearly mapping each patch (that is now an 8-dimensional vector in our example) to 3 distinct vectors: q, k, and v (query, key, value).\n",
    "\n",
    "Then, for a single patch, we are going to compute the dot product between its q vector with all of the k vectors, divide by the square root of the dimensionality of these vectors (sqrt(8)), softmax these so-called attention cues, and finally multiply each attention cue with the v vectors associated with the different k vectors and sum all up.\n",
    "\n",
    "In this way, each patch assumes a new value that is based on its similarity (after the linear mapping to q, k, and v) with other patches. This whole procedure, however, is carried out H times on H sub-vectors of our current 8-dimensional patches, where H is the number of Heads.\n",
    "\n",
    "See https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d  # token_dim = 8\n",
    "        self.n_heads = n_heads\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)  # 4\n",
    "        self.q_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequence has shape (N, seq_length=50, token_dim=8)\n",
    "        # We go into shape   (N, seq_length=50, n_heads=2, token_dim / n_heads=4)\n",
    "        # And come back to   (N, seq_length=50, item_dim) through concatenation\n",
    "        result = []\n",
    "        for sequence in sequences:  # sequences are the image patches\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                # mapping: 4x4\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                # seq has shape (seq_length=50, d_head=4)\n",
    "                seq = sequence[:, head * self.d_head: (head+1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                # (50,4)*(4,50) = (50,50)\n",
    "                attention = self.softmax(q @ k.T / self.d_head ** 0.5)\n",
    "\n",
    "                # (50,50) * (4,50) = (50, 4)\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.normal = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.normal(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw=(1, 28, 28), n_patches=7, hidden_d=8, n_blocks=2, n_heads=2, out_d=10):\n",
    "        super(MyViT, self).__init__()\n",
    "        self.chw = chw\n",
    "        self.n_patches = n_patches\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_blocks = n_blocks  # Encoder Blocks\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear Mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        # stacked before of the input embeddings\n",
    "        # torch.Size([7, 49, 8])\n",
    "        # torch.Size([7, 50, 8])\n",
    "        # It will capture information about the other tokens\n",
    "        # When information about all other tokens will be present here,\n",
    "        # we will be able to classify the image using only this special token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional encoding\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings(\n",
    "            self.n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "\n",
    "        # 4) Transformer encoder\n",
    "        self.blocks = nn.ModuleList(MyViTBlock(\n",
    "            hidden_d, n_heads) for _ in range(n_blocks))\n",
    "\n",
    "        # 5) Classification using the classification token only\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, images):\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Tokenize the image into shape(N, 49, 8)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        print(\"tokens.shape\", tokens.shape)\n",
    "\n",
    "        # Adding classification token\n",
    "        # (N, 50, 8)\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "        print(\"tokens.shape after adding class token\", tokens.shape)\n",
    "\n",
    "        # Add positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer\n",
    "        # (N, 50, 8)\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # Get Classification token\n",
    "        out = out[:, 0]\n",
    "        out = self.mlp(out)\n",
    "        print(\"mlp.shape \", out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    transform = ToTensor()\n",
    "    train_set = MNIST(root='./datasets', train=True,\n",
    "                      download=True, transform=transform)\n",
    "    test_set = MNIST(root='./datasets', train=False,\n",
    "                     download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=512)\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=512)\n",
    "    device = torch.device(\"mps\")\n",
    "    model = MyViT((1, 28, 28), n_patches=7, n_blocks=2,\n",
    "                  hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "    N_EPOCHS = 5\n",
    "    LR = 0.005\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in trange(N_EPOCHS, desc=\"training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1} in training', leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1)\n",
    "                                 == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    plt.imshow(get_positional_embeddings(49+1, 8),\n",
    "               cmap=\"hot\", interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "\n",
    "    block = MyViTBlock(hidden_d=8, n_heads=2)\n",
    "    y = torch.randn(7, 50, 8)\n",
    "    block(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* [Vision Transformers from Scratch (PyTorch): A step-by-step guide](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
